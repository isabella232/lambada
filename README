Lambada - Automated Deployment of Python Methods to the (Lambda/Multi-FaaS) Cloud
Copyright (C) 2016-18 Josef Spillner <josef.spillner@zhaw.ch>
          (C) 2019 Zurich University of Applied Sciences
with contributions by Josef Spillner and Daiana Boruta
=========================================================================

Lambada is an executable Python module which dynamically re-wites any
module or script it is included in or invoked with into cloud-hosted
code. This means that all functions become externalised (if feasible),
deployed to a FaaS provider and automatically executed there, or
rewritten as deployable units into local files. Furthermore, classes are
wrapped into proxy classes for stateful access to individual methods as
functions.

Supported FaaS providers: AWS Lambda, IBM Cloud Functions (+ OpenWhisk),
Google Cloud Functions. Select via --provider {lambda,whisk,ibm,google}.

Using function decorators in conjunction with the command line option
--annotations, functions can be turned into cloud functions selectively
and furthermore run with custom configuration beyond the default. The
two kinds of decorators to place on top of a Python function definition
are:
- @cloudfunction
- @cloudfunction(memory=X [MB],duration=X [s])
Have a look at examples/decorated.py to copy the decorator code into
your application.

General instructions:
- Install Python 3.5+

Instructions for local mode:
- Run Lambada with --local (and if you prefer with --debug also)
- It will create copies of files suffixed with _lambdafied.py

Instructions for local/remote mode and self-hosted Snafu or OpenWhisk:
- Run Lambada with --endpoint http://localhost:10000
- It will deploy function packages into the given instance of Snafu or
  OpenWhisk

Instructions for remote mode and AWS, IBM or Google:
- Create an account at the cloud provider
- Configure the CLI tools so that listing functions works
  * for Lambda: `aws lambda list-functions'
  * for IBM: `ibmcloud fn list'
  * etc.
- Configure to have at least one dedicated execution role, and enter its
  unique number (ARN) specifically in AWS
  * as environment variable: LAMBADAROLEARN=..., e.g. through wrapper
    script `run.sh'
  * as parameter to lambada.move(lambdarolearn=...)
  * or simply leave it out and Lambada will auto-detect your role
- Now you can run Lambada without --local and deploy directly to the
  FaaS provider

Interesting files:
- lambadalib/lambada.py: main module file
- lambadalib/lambadanew.py: refactored main module file
- lambadalib/functionproxy.py: class proxy wrapper
- examples/myapp.py: sample lambada-fied application script
- examples/myapp-tests.py: full unit test coverage for the sample script
- examples/values.csv: sample data for the sample script
- examples/fib.py: another application script, not internally
  lambada-fied
- examples/decorated.py: how to use decorators

Not so interesting files:
- lambadalib/codegen.py: external, from
  https://github.com/andreif/codegen, with some limitations, e.g. `with'
- examples/functions/: directory with some testing scripts, can be
  ignored
- README: you're reading it right now

More information on the design and implementation of (a previous version
of) Lambada is available in a preprint: https://arxiv.org/abs/1705.08169

Have a look at our ongoing serverless research!
https://blog.zhaw.ch/splab/tag/serverless/
